apiVersion: v1
kind: ConfigMap
metadata:
  name: fastmcp-config
  namespace: fastmcp-server
data:
  requirements.txt: |
    fastmcp
    requests
    uvicorn
    fastapi
    pydantic
  server.py: |
    from fastapi import FastAPI, HTTPException
    from fastapi.responses import JSONResponse
    from fastmcp import FastMCP
    import requests
    import os
    from typing import List, Optional, Dict, Any, Literal
    from pydantic import BaseModel, Field

    app = FastAPI()
    mcp = FastMCP() 

    OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL", "http://ollama.local:11434")

    # --- CORREÇÃO: Definir modelo Pydantic para o parâmetro 'messages' ---
    class Message(BaseModel):
        role: Literal["system", "user", "assistant", "tool"]
        content: str
        images: Optional[List[str]] = Field(default=None, description="Lista de imagens codificadas em base64.")
    # --- Fim da Correção ---

    # Helper function to make requests to Ollama API
    def call_ollama_api(endpoint, method='POST', json_data=None, stream=False):
        url = f"{OLLAMA_SERVER_URL}{endpoint}"
        headers = {'Content-Type': 'application/json'}
        try:
            if method == 'POST':
                response = requests.post(url, headers=headers, json=json_data, stream=stream)
            elif method == 'GET':
                response = requests.get(url, headers=headers, stream=stream)
            else:
                raise ValueError("Unsupported HTTP method")
            response.raise_for_status()
            if stream:
                return response.iter_content(chunk_size=8192)
            return response.json()
        except requests.exceptions.RequestException as e:
            raise HTTPException(status_code=500, detail=f"Ollama API error: {e}")

    # Endpoints da API do Ollama como Tools

    @mcp.tool(
        name="generate_completion",
        description="Gera uma conclusão para um dado prompt com um modelo Ollama."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def generate_completion(model: str, prompt: str, stream: bool = False, options: Optional[Dict[str, Any]] = None):
        payload = {"model": model, "prompt": prompt, "stream": stream}
        if options: payload["options"] = options
        return call_ollama_api("/api/generate", json_data=payload, stream=stream)

    @mcp.tool(
        name="generate_chat_completion",
        description="Gera a próxima mensagem em um chat com um modelo Ollama, mantendo o histórico de mensagens."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def generate_chat_completion(model: str, messages: List[Message], stream: bool = False, options: Optional[Dict[str, Any]] = None):
        # A validação de 'messages' agora é feita pelo Pydantic
        payload = {"model": model, "messages": [msg.dict(exclude_none=True) for msg in messages], "stream": stream}
        if options: payload["options"] = options
        return call_ollama_api("/api/chat", json_data=payload, stream=stream)

    @mcp.tool(
        name="create_model",
        description="Cria um modelo Ollama a partir de outro modelo, diretório safetensors ou arquivo GGUF."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def create_model(model: str, from_model: Optional[str] = None, quantize: Optional[str] = None, system: Optional[str] = None):
        payload = {"model": model}
        if from_model: payload["from"] = from_model
        if quantize: payload["quantize"] = quantize
        if system: payload["system"] = system
        return call_ollama_api("/api/create", json_data=payload)

    @mcp.tool(
        name="list_local_models",
        description="Lista os modelos Ollama disponíveis localmente."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def list_local_models():
        return call_ollama_api("/api/tags", method='GET')

    @mcp.tool(
        name="show_model_information",
        description="Mostra informações sobre um modelo Ollama, incluindo detalhes, modelfile, template, parâmetros, licença e prompt do sistema."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def show_model_information(model: str, verbose: bool = False):
        payload = {"model": model, "verbose": verbose}
        return call_ollama_api("/api/show", json_data=payload)

    @mcp.tool(
        name="copy_model",
        description="Copia um modelo Ollama existente para um novo nome."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def copy_model(source_model: str, destination_model: str):
        payload = {"source": source_model, "destination": destination_model}
        return call_ollama_api("/api/copy", json_data=payload)

    @mcp.tool(
        name="delete_model",
        description="Exclui um modelo Ollama."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def delete_model(model: str):
        payload = {"name": model}
        return call_ollama_api("/api/delete", method='DELETE', json_data=payload)

    @mcp.tool(
        name="pull_model",
        description="Baixa um modelo Ollama do registro."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def pull_model(model: str, insecure: bool = False, stream: bool = False):
        payload = {"name": model, "insecure": insecure, "stream": stream}
        return call_ollama_api("/api/pull", json_data=payload, stream=stream)

    @mcp.tool(
        name="push_model",
        description="Envia um modelo Ollama para um registro."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def push_model(model: str, insecure: bool = False, stream: bool = False):
        payload = {"name": model, "insecure": insecure, "stream": stream}
        return call_ollama_api("/api/push", json_data=payload, stream=stream)

    @mcp.tool(
        name="generate_embeddings",
        description="Gera embeddings a partir de um modelo Ollama."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def generate_embeddings(model: str, prompt: str):
        payload = {"model": model, "prompt": prompt}
        return call_ollama_api("/api/embeddings", json_data=payload)

    @mcp.tool(
        name="list_running_models",
        description="Lista os modelos Ollama que estão atualmente carregados na memória."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def list_running_models():
        return call_ollama_api("/api/ps", method='GET')

    @mcp.tool(
        name="get_ollama_version",
        description="Recupera a versão do Ollama."
        # --- CORREÇÃO: Bloco 'parameters' removido ---
    )
    async def get_ollama_version():
        return call_ollama_api("/api/version", method='GET')

    # Monte o servidor MCP no seu app FastAPI
    mcp_app = mcp.http_app()
    app.mount("/", mcp_app)

    if __name__ == "__main__":
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000)