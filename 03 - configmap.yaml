apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  # Instala as dependências de servidor e de linha de comando
  requirements.txt: |
    mcp[server,cli]
    httpx
    fastapi

  # Servidor MCP com o conjunto completo de tools do Ollama
  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional

    # --- INÍCIO DA CORREÇÃO ---
    from fastapi import FastAPI
    from mcp.server.fastmcp import FastMCP
    from pantic import BaseModel, Field

    # 1. Crie o gerenciador MCP
    mcp_manager = FastMCP("Ollama Tools Server")

    # 2. Exponha o aplicativo FastAPI interno com o nome 'app'
    app = mcp_manager.fastapi
    # --- FIM DA CORREÇÃO ---

    # Pega a URL do Ollama da variável de ambiente
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")

    # Cria um cliente HTTP assíncrono para se comunicar com o Ollama
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")

    # --- Schemas de Input (Pydantic) para as Tools ---

    class OllamaGenerateInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        prompt: str = Field(description="O prompt para gerar uma resposta.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")
        system: Optional[str] = Field(None, description="A instrução de sistema (system prompt).")
        template: Optional[str] = Field(None, description="O template de prompt a ser usado (sobrescreve o do modelo).")
        context: Optional[str] = Field(None, description="O contexto de uma conversa anterior (para manter estado).")

    class OllamaChatInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        messages: List[Dict[str, str]] = Field(description="Lista de mensagens no formato {'role': 'user', 'content': '...'}.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")

    class OllamaCreateInput(BaseModel):
        name: str = Field(description="O nome do novo modelo a ser criado (ex: 'meu-modelo:latest').")
        modelfile: str = Field(description="O conteúdo completo do 'Modelfile' como uma string.")

    class OllamaModelInfoInput(BaseModel):
        name: str = Field(description="O nome do modelo para inspecionar.")

    class OllamaCopyInput(BaseModel):
        source: str = Field(description="O nome do modelo de origem (ex: 'llama3').")
        destination: str = Field(description="O novo nome para a cópia (ex: 'llama3-backup').")

    class OllamaDeleteInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser excluído permanentemente.")

    class OllamaPullInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser baixado do registro (ex: 'llama3:latest').")
        insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")

    class OllamaPushInput(BaseModel):
        name: str = Field(description="O nome do modelo a