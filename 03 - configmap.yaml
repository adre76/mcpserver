apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  requirements.txt: |
    fastmcp
    httpx
    pydantic
  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional
    from fastmcp import FastMCP
    from pydantic import BaseModel, Field

    mcp = FastMCP(name="Ollama Tools Server")
    OLLAMA_BASE_URL = os.getenv("OLLAMABASEURL", "http://ollama.local")
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print("MCP Server Ollama Full API iniciado. Apontando para:", OLLAMA_BASE_URL)

    # Modelos Pydantic usados para validação dos dados
    # Exemplo de um modelo (detalhe cortado para foco):

    class OllamaGenerateInput(BaseModel):
        model: str = Field(..., description="O nome do modelo a ser usado ex llama3latest.")
        prompt: str = Field(..., description="O prompt para geração de texto.")
        format: Optional[str] = None
        options: Optional[Dict[str, Any]] = None
        system: Optional[str] = None
        template: Optional[str] = None

    async def _handle_ollama_request(http_method: str, url_path: str, payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        print(f"Fazendo requisição {http_method} para {url_path}")
        response = await ollama_client.request(method=http_method, url=url_path, json=payload)
        response.raise_for_status()
        return response.json()

    # --- Tools ---
    print("Iniciando registro dos tools MCP...")

    @mcp.tool
    async def ping():
        print("Ping tool executado")
        return {"pong": True}

    @mcp.tool()
    async def ollama_generate(data: OllamaGenerateInput) -> Dict[str, Any]:
        """Gera uma conclusão de texto usando o modelo Ollama."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/generate", payload)

    @mcp.tool()
    async def ollama_chat(data: OllamaChatInput) -> Dict[str, Any]:
        """Gera uma resposta de chat baseada nas mensagens."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/chat", payload)

    @mcp.tool()
    async def ollama_create_model(data: OllamaCreateInput) -> Dict[str, Any]:
        """Cria um novo modelo Ollama."""
        payload = data.model_dump()
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/create", payload)

    @mcp.tool()
    async def ollamalistmodels() -> Dict[str, Any]:
        """Lista todos os modelos disponíveis."""
        return await _handle_ollama_request("GET", "/api/tags")

    @mcp.tool()
    async def ollama_show_model_info(data: OllamaModelInfoInput) -> Dict[str, Any]:
        """Exibe informações detalhadas de um modelo."""
        return await _handle_ollama_request("POST", "/api/show", data.model_dump())

    @mcp.tool()
    async def ollama_copy_model(data: OllamaCopyInput) -> Dict[str, Any]:
        """Cria uma cópia de um modelo existente."""
        return await _handle_ollama_request("POST", "/api/copy", data.model_dump())

    @mcp.tool()
    async def ollama_delete_model(data: OllamaDeleteInput) -> Dict[str, Any]:
        """Exclui permanentemente um modelo."""
        return await _handle_ollama_request("DELETE", "/api/delete", data.model_dump())

    @mcp.tool()
    async def ollama_pull_model(data: OllamaPullInput) -> Dict[str, Any]:
        """Baixa um modelo do registro."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/pull", payload)

    @mcp.tool()
    async def ollama_push_model(data: OllamaPushInput) -> Dict[str, Any]:
        """Envia um modelo local para o registro."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/push", payload)

    @mcp.tool()
    async def ollama_generate_embeddings(data: OllamaEmbeddingsInput) -> Dict[str, Any]:
        """Gera representações vetoriais (embeddings)."""
        payload = data.model_dump(exclude_none=True)
        return await _handle_ollama_request("POST", "/api/embeddings", payload)

    @mcp.tool()
    async def ollama_list_running_models() -> Dict[str, Any]:
        """Lista os modelos em execução."""
        return await _handle_ollama_request("GET", "/api/ps")

    @mcp.tool()
    async def ollama_get_version() -> Dict[str, Any]:
        """Retorna a versão do servidor Ollama."""
        return await _handle_ollama_request("GET", "/api/version")

    print("Registro dos tools MCP finalizado")