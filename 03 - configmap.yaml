apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  requirements.txt: |
    fastmcp
    httpx
    pydantic

  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional

    from fastmcp import FastMCP
    from pydantic import BaseModel, Field
    
    mcp = FastMCP("Ollama Tools Server")
    
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")

    # --- Schemas ---
    class OllamaGenerateInput(BaseModel): model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest')."); prompt: str = Field(description="O prompt para gerar uma resposta."); format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json')."); options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature')."); system: Optional[str] = Field(None, description="A instrução de sistema (system prompt)."); template: Optional[str] = Field(None, description="O template de prompt a ser usado (sobrescreve o do modelo)."); context: Optional[str] = Field(None, description="O contexto de uma conversa anterior (para manter estado).")
    class OllamaChatInput(BaseModel): model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest')."); messages: List[Dict[str, str]] = Field(description="Lista de mensagens no formato {'role': 'user', 'content': '...'}."); format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json')."); options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")
    class OllamaCreateInput(BaseModel): name: str = Field(description="O nome do novo modelo a ser criado (ex: 'meu-modelo:latest')."); modelfile: str = Field(description="O conteúdo completo do 'Modelfile' como uma string.")
    class OllamaModelInfoInput(BaseModel): name: str = Field(description="O nome do modelo a ser inspecionar.")
    class OllamaCopyInput(BaseModel): source: str = Field(description="O nome do modelo de origem (ex: 'llama3')."); destination: str = Field(description="O novo nome para a cópia (ex: 'llama3-backup').")
    class OllamaDeleteInput(BaseModel): name: str = Field(description="O nome do modelo a ser excluído permanentemente.")
    class OllamaPullInput(BaseModel): name: str = Field(description="O nome do modelo a ser baixado do registro (ex: 'llama3:latest')."); insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")
    class OllamaPushInput(BaseModel): name: str = Field(description="O nome do modelo a ser enviado para o registro (ex: 'meu-namespace/meu-modelo')."); insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")
    class OllamaEmbeddingsInput(BaseModel): model: str = Field(description="O nome do modelo de embedding a ser usado."); prompt: str = Field(description="O texto a ser convertido em um vetor (embedding)."); options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo.")

    # --- Bloco de Erro ---
    async def _handle_ollama_request( method: str, path: str, payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        try:
            if method == "GET": response = await ollama_client.get(path)
            elif method == "POST": response = await ollama_client.post(path, json=payload)
            elif method == "DELETE": response = await ollama_client.request("DELETE", path, json=payload)
            else: return {"error": f"MCP Tool: Método HTTP não suportado: {method}"}
            response.raise_for_status()
            if response.status_code == 200 and not response.content: return {"status": "success"}
            if path == "/api/version": return {"version": response.text.strip()}
            return response.json()
        except httpx.ConnectError as e: return {"error": f"MCP Tool: Não foi possível conectar ao Ollama em {OLLAMA_BASE_URL}", "details": str(e)}
        except httpx.HTTPStatusError as e: return {"error": f"MCP Tool: Ollama retornou um erro {e.response.status_code}", "details": e.response.text or "Sem detalhes"}
        except Exception as e: return {"error": f"MCP Tool: Erro inesperado ao processar {path}", "details": str(e)}

    # --- Tools ---
    @mcp.tool()
    async def ollama_generate(data: OllamaGenerateInput) -> Dict[str, Any]: """Gera uma conclusão de texto..."""; payload = data.model_dump(exclude_none=True); payload["stream"] = False; return await _handle_ollama_request("POST", "/api/generate", payload)
    @mcp.tool()
    async def ollama_chat(data: OllamaChatInput) -> Dict[str, Any]: """Gera uma resposta de chat..."""; payload = data.model_dump(exclude_none=True); payload["stream"] = False; return await _handle_ollama_request("POST", "/api/chat", payload)
    @mcp.tool()
    async def ollama_create_model(data: OllamaCreateInput) -> Dict[str, Any]: """Cria um novo modelo Ollama..."""; payload = data.model_dump(); payload["stream"] = False; return await _handle_ollama_request("POST", "/api/create", payload)
    @mcp.tool(name="lista modelos", description="Lista todos os modelos LLM disponíveis no Ollama.", tags={"listar", "modelos"}, meta={"version": "1.0", "author": "Andre Pereira"})
    async def ollama_list_models() -> Dict[str, Any]: """Lista todos os modelos..."""; return await _handle_ollama_request("GET", "/api/tags")
    @mcp.tool()
    async def ollama_show_model_info(data: OllamaModelInfoInput) -> Dict[str, Any]: """Exibe informações detalhadas..."""; return await _handle_ollama_request("POST", "/api/show", data.model_dump())
    @mcp.tool()
    async def ollama_copy_model(data: OllamaCopyInput) -> Dict[str, Any]: """Cria uma cópia de um modelo..."""; return await _handle_ollama_request("POST", "/api/copy", data.model_dump())
    @mcp.tool()
    async def ollama_delete_model(data: OllamaDeleteInput) -> Dict[str, Any]: """Exclui permanentemente um modelo..."""; return await _handle_ollama_request("DELETE", "/api/delete", data.model_dump())
    @mcp.tool()
    async def ollama_pull_model(data: OllamaPullInput) -> Dict[str, Any]: """Baixa um modelo..."""; payload = data.model_dump(exclude_none=True); payload["stream"] = False; return await _handle_ollama_request("POST", "/api/pull", payload)
    @mcp.tool()
    async def ollama_push_model(data: OllamaPushInput) -> Dict[str, Any]: """Envia um modelo local..."""; payload = data.model_dump(exclude_none=True); payload["stream"] = False; return await _handle_ollama_request("POST", "/api/push", payload)
    @mcp.tool()
    async def ollama_generate_embeddings(data: OllamaEmbeddingsInput) -> Dict[str, Any]: """Gera representações vetoriais..."""; payload = data.model_dump(exclude_none=True); return await _handle_ollama_request("POST", "/api/embeddings", payload)
    @mcp.tool(())
    async def ollama_list_running_models() -> Dict[str, Any]: """Verifica e lista quais modelos..."""; return await _handle_ollama_request("GET", "/api/ps")
    @mcp.tool()
    async def ollama_get_version() -> Dict[str, Any]: """Retorna a string de versão..."""; return await _handle_ollama_request("GET", "/api/version")

    # --- Iniciar servidor ---
    if __name__ == "__main__":
        print("Iniciando servidor FastMCP HTTP via mcp.run()...")
        mcp.run(transport="http", host="0.0.0.0", port=8000, path="/mcp")