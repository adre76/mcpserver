apiVersion: v1
kind: ConfigMap
metadata:
  name: fastmcp-config
  namespace: fastmcp-server
data:
  requirements.txt: |
    fastmcp
    requests
    uvicorn
    fastapi
    pydantic
  server.py: |
    import sys
    print("Iniciando server.py com correcao EXPLICITA de argumentos...", file=sys.stderr)

    from fastapi import FastAPI, HTTPException
    try:
        from fastmcp import FastMCP
        import requests
        import os
        from typing import List, Optional, Dict, Any, Literal
        from pydantic import BaseModel, Field, ConfigDict
    except ImportError as e:
        print(f"CRITICAL: Failed imports: {e}", file=sys.stderr)
        sys.exit(1)

    mcp = FastMCP(stateless_http=True)
    OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL", "http://ollama.local")

    # --- 1. Modelos Base (Whitelist n8n) ---
    class BaseIgnorer(BaseModel):
        model_config = ConfigDict(extra='ignore')
        # Definimos estes campos para que as ferramentas complexas (que usam Modelos) os aceitem
        sessionId: Optional[str] = Field(default=None, description="Ignored")
        action: Optional[str] = Field(default=None, description="Ignored")
        chatInput: Optional[str] = Field(default=None, description="Ignored")
        toolCallId: Optional[str] = Field(default=None, description="Ignored")

    class Message(BaseIgnorer):
        role: Literal["system", "user", "assistant", "tool"] = Field(..., description="Papel.")
        content: str = Field(..., description="Conteúdo.")
        images: Optional[List[str]] = Field(default=None, description="Imagens.")

    class GenerateCompletionInput(BaseIgnorer):
        model: str = Field(..., description="Modelo.")
        prompt: str = Field(..., description="Prompt.")
        stream: bool = Field(default=False, description="Stream.")
        options: Optional[Dict[str, Any]] = Field(default=None, description="Opções.")

    class GenerateChatInput(BaseIgnorer):
        model: str = Field(..., description="Modelo.")
        messages: List[Message] = Field(..., description="Histórico.")
        stream: bool = Field(default=False, description="Stream.")
        options: Optional[Dict[str, Any]] = Field(default=None, description="Opções.")

    class ModelInput(BaseIgnorer):
        model: str = Field(..., description="Nome do modelo.")

    class CopyModelInput(BaseIgnorer):
        source_model: str = Field(..., description="Origem.")
        destination_model: str = Field(..., description="Destino.")

    class CreateModelInput(BaseIgnorer):
        model: str = Field(..., description="Nome.")
        from_model: str = Field(..., description="Base.")
        system: Optional[str] = Field(default=None, description="System Prompt.")
        quantize: Optional[str] = Field(default=None, description="Quantização.")

    # --- 2. Helper ---
    def call_ollama(endpoint, json_data=None, method='POST', stream=False):
        url = f"{OLLAMA_SERVER_URL}{endpoint}"
        try:
            # Removemos campos do n8n antes de enviar ao Ollama para não sujar o log ou request
            if json_data:
                json_data = {k: v for k, v in json_data.items() if k not in ['sessionId', 'action', 'chatInput', 'toolCallId']}
            
            if method == 'GET':
                r = requests.get(url, headers={'Content-Type': 'application/json'}, stream=stream)
            elif method == 'DELETE':
                r = requests.delete(url, headers={'Content-Type': 'application/json'}, json=json_data)
            else:
                r = requests.post(url, headers={'Content-Type': 'application/json'}, json=json_data, stream=stream)
            r.raise_for_status()
            return r.iter_content(chunk_size=8192) if stream else r.json()
        except Exception as e:
            print(f"Erro Ollama: {e}", file=sys.stderr)
            raise HTTPException(status_code=500, detail=f"Ollama Error: {str(e)}")

    # --- 3. Ferramentas (Tools) ---

    # Ferramentas COMPLEXAS (Usa Modelo Pydantic -> BaseIgnorer cuida dos campos extras)
    @mcp.tool(name="generate_completion")
    async def generate_completion(input_data: GenerateCompletionInput):
        return call_ollama("/api/generate", json_data=input_data.model_dump(exclude_none=True), stream=input_data.stream)

    @mcp.tool(name="generate_chat_completion")
    async def generate_chat_completion(input_data: GenerateChatInput):
        d = input_data.model_dump(exclude_none=True)
        # Limpeza extra nas mensagens aninhadas
        if 'messages' in d:
             d["messages"] = [{k: v for k, v in m.items() if k not in ['sessionId','action','chatInput','toolCallId']} for m in d["messages"]]
        return call_ollama("/api/chat", json_data=d, stream=input_data.stream)

    @mcp.tool(name="pull_model")
    async def pull_model(input_data: ModelInput):
        return call_ollama("/api/pull", json_data=input_data.model_dump(exclude_none=True))
    
    @mcp.tool(name="delete_model")
    async def delete_model(input_data: ModelInput):
        return call_ollama("/api/delete", method='DELETE', json_data=input_data.model_dump(exclude_none=True))

    @mcp.tool(name="show_model_information")
    async def show_model_information(input_data: ModelInput):
        return call_ollama("/api/show", json_data=input_data.model_dump(exclude_none=True))

    @mcp.tool(name="copy_model")
    async def copy_model(input_data: CopyModelInput):
        payload = {"source": input_data.source_model, "destination": input_data.destination_model}
        return call_ollama("/api/copy", json_data=payload)
    
    @mcp.tool(name="create_model")
    async def create_model(input_data: CreateModelInput):
        modelfile = f"FROM {input_data.from_model}\n"
        if input_data.system:
            modelfile += f'SYSTEM """{input_data.system}"""\n'
        if input_data.quantize:
            modelfile += f"PARAMETER quantize {input_data.quantize}\n"
        payload = {"name": input_data.model, "modelfile": modelfile}
        return call_ollama("/api/create", json_data=payload)

    # --- CORREÇÃO CRÍTICA AQUI ---
    # Ferramentas SIMPLES (Sem input de usuário real)
    # Definimos explicitamente os argumentos do n8n para que a validação passe.
    
    @mcp.tool(name="list_local_models")
    async def list_local_models(sessionId: str = None, action: str = None, chatInput: str = None, toolCallId: str = None):
        return call_ollama("/api/tags", method='GET')

    @mcp.tool(name="list_running_models")
    async def list_running_models(sessionId: str = None, action: str = None, chatInput: str = None, toolCallId: str = None):
        return call_ollama("/api/ps", method='GET')

    @mcp.tool(name="get_ollama_version")
    async def get_ollama_version(sessionId: str = None, action: str = None, chatInput: str = None, toolCallId: str = None):
        return call_ollama("/api/version", method='GET')


    # --- 4. Inicialização ---
    mcp_app = mcp.http_app(path="/")
    app = FastAPI(lifespan=mcp_app.lifespan)
    app.mount("/mcp", mcp_app)

    if __name__ == "__main__":
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000)