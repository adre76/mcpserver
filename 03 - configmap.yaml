apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  requirements.txt: |
    fastmcp
    httpx
    pydantic
    fastapi
    uvicorn # Adicionar uvicorn aqui explicitamente

  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional

    # --- INÍCIO DA CORREÇÃO ---
    from fastapi import FastAPI
    from fastmcp import FastMCP
    from pydantic import BaseModel, Field
    import uvicorn # Importar uvicorn

    # 1. Crie o aplicativo FastAPI principal
    app = FastAPI()

    # 2. Crie o gerenciador MCP (sem passar o app ainda)
    mcp_manager = FastMCP("Ollama Tools Server")

    # 3. Monte o aplicativo interno do FastMCP sob o prefixo /mcp
    #    (Assumindo que o app interno está em .fastapi)
    #    Se isso der AttributeError, tentaremos .app
    if hasattr(mcp_manager, 'fastapi'):
        app.mount("/mcp", mcp_manager.fastapi)
        print("Montado mcp_manager.fastapi em /mcp")
    elif hasattr(mcp_manager, 'app'):
        app.mount("/mcp", mcp_manager.app)
        print("Montado mcp_manager.app em /mcp")
    else:
        print("ERRO: Não foi possível encontrar o app interno do FastMCP (.fastapi ou .app)")
        # Você pode querer lançar uma exceção aqui se preferir
        import sys
        sys.exit("Falha ao montar o app FastMCP")

    # --- FIM DA CORREÇÃO ---

    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")

    # --- Schemas ---
    class OllamaGenerateInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        prompt: str = Field(description="O prompt para gerar uma resposta.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")
        system: Optional[str] = Field(None, description="A instrução de sistema (system prompt).")
        template: Optional[str] = Field(None, description="O template de prompt a ser usado (sobrescreve o do modelo).")
        context: Optional[str] = Field(None, description="O contexto de uma conversa anterior (para manter estado).")

    # ... (Restante das classes Pydantic - OllamaChatInput, etc. - sem alterações) ...
    class OllamaChatInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        messages: List[Dict[str, str]] = Field(description="Lista de mensagens no formato {'role': 'user', 'content': '...'}.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")
    class OllamaCreateInput(BaseModel):
        name: str = Field(description="O nome do novo modelo a ser criado (ex: 'meu-modelo:latest').")
        modelfile: str = Field(description="O conteúdo completo do 'Modelfile' como uma string.")
    class OllamaModelInfoInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser inspecionar.")
    class OllamaCopyInput(BaseModel):
        source: str = Field(description="O nome do modelo de origem (ex: 'llama3').")
        destination: str = Field(description="O novo nome para a cópia (ex: 'llama3-backup').")
    class OllamaDeleteInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser excluído permanentemente.")
    class OllamaPullInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser baixado do registro (ex: 'llama3:latest').")
        insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")
    class OllamaPushInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser enviado para o registro (ex: 'meu-namespace/meu-modelo').")
        insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")
    class OllamaEmbeddingsInput(BaseModel):
        model: str = Field(description="O nome do modelo de embedding a ser usado.")
        prompt: str = Field(description="O texto a ser convertido em um vetor (embedding).")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo.")


    # --- Bloco de Erro ---
    async def _handle_ollama_request(
        method: str,
        path: str,
        payload: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        # ... (código _handle_ollama_request sem alterações) ...
        try:
            if method == "GET":
                response = await ollama_client.get(path)
            elif method == "POST":
                response = await ollama_client.post(path, json=payload)
            elif method == "DELETE":
                response = await ollama_client.request("DELETE", path, json=payload)
            else:
                return {"error": f"MCP Tool: Método HTTP não suportado: {method}"}
            response.raise_for_status()
            if response.status_code == 200 and not response.content: return {"status": "success"}
            if path == "/api/version": return {"version": response.text.strip()}
            return response.json()
        except httpx.ConnectError as e: return {"error": f"MCP Tool: Não foi possível conectar ao Ollama em {OLLAMA_BASE_URL}", "details": str(e)}
        except httpx.HTTPStatusError as e: return {"error": f"MCP Tool: Ollama retornou um erro {e.response.status_code}", "details": e.response.text or "Sem detalhes"}
        except Exception as e: return {"error": f"MCP Tool: Erro inesperado ao processar {path}", "details": str(e)}

    # --- Definição das Tools ---
    # (Usando @mcp_manager.tool)

    @mcp_manager.tool()
    async def ollama_generate(data: OllamaGenerateInput) -> Dict[str, Any]:
        """Gera uma conclusão de texto (completion) a partir de um único prompt..."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/generate", payload)

    # ... (Restante das tools @mcp_manager.tool - sem alterações) ...
    @mcp_manager.tool()
    async def ollama_chat(data: OllamaChatInput) -> Dict[str, Any]:
        """Gera uma resposta de chat baseada em um histórico de mensagens..."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/chat", payload)
    @mcp_manager.tool()
    async def ollama_create_model(data: OllamaCreateInput) -> Dict[str, Any]:
        """Cria um novo modelo Ollama customizado a partir do conteúdo de um 'Modelfile'..."""
        payload = data.model_dump()
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/create", payload)
    @mcp_manager.tool()
    async def ollama_list_models() -> Dict[str, Any]:
        """Lista todos os modelos de linguagem baixados..."""
        return await _handle_ollama_request("GET", "/api/tags")
    @mcp_manager.tool()
    async def ollama_show_model_info(data: OllamaModelInfoInput) -> Dict[str, Any]:
        """Exibe informações detalhadas sobre um modelo específico..."""
        return await _handle_ollama_request("POST", "/api/show", data.model_dump())
    @mcp_manager.tool()
    async def ollama_copy_model(data: OllamaCopyInput) -> Dict[str, Any]:
        """Cria uma cópia de um modelo local existente..."""
        return await _handle_ollama_request("POST", "/api/copy", data.model_dump())
    @mcp_manager.tool()
    async def ollama_delete_model(data: OllamaDeleteInput) -> Dict[str, Any]:
        """Exclui permanentemente um modelo do servidor Ollama local..."""
        return await _handle_ollama_request("DELETE", "/api/delete", data.model_dump())
    @mcp_manager.tool()
    async def ollama_pull_model(data: OllamaPullInput) -> Dict[str, Any]:
        """Baixa um modelo de um registro (como o Ollama Hub)..."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/pull", payload)
    @mcp_manager.tool()
    async def ollama_push_model(data: OllamaPushInput) -> Dict[str, Any]:
        """Envia um modelo local para um registro remoto..."""
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/push", payload)
    @mcp_manager.tool()
    async def ollama_generate_embeddings(data: OllamaEmbeddingsInput) -> Dict[str, Any]:
        """Gera representações vetoriais (embeddings)..."""
        payload = data.model_dump(exclude_none=True)
        return await _handle_ollama_request("POST", "/api/embeddings", payload)
    @mcp_manager.tool()
    async def ollama_list_running_models() -> Dict[str, Any]:
        """Verifica e lista quais modelos estão atualmente carregados na memória..."""
        return await _handle_ollama_request("GET", "/api/ps")
    @mcp_manager.tool()
    async def ollama_get_version() -> Dict[str, Any]:
        """Retorna a string de versão exata do servidor Ollama em execução."""
        return await _handle_ollama_request("GET", "/api/version")


    # --- Bloco para iniciar o servidor ---
    if __name__ == "__main__":
        print("Iniciando servidor Uvicorn com app FastAPI montado...")
        # Usa uvicorn para rodar o app FastAPI principal
        uvicorn.run(app, host="0.0.0.0", port=8000)