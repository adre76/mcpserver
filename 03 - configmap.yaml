apiVersion: v1
kind: ConfigMap
metadata:
  name: fastmcp-config
  namespace: fastmcp-server
data:
  requirements.txt: |
    fastmcp
    requests
    uvicorn
    fastapi
    pydantic
  server.py: |
    from fastapi import FastAPI, HTTPException
    from fastmcp import FastMCP
    import requests
    import os
    from typing import List, Optional, Dict, Any, Literal
    from pydantic import BaseModel, Field

    # --- 1. Crie a instância MCP PRIMEIRO ---
    mcp = FastMCP() 

    OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL", "http://ollama.local:11434")

    # --- Definição do Modelo Pydantic ---
    class Message(BaseModel):
        role: Literal["system", "user", "assistant", "tool"]
        content: str
        images: Optional[List[str]] = Field(default=None, description="Lista de imagens codificadas em base64.")

    # --- Função Helper ---
    def call_ollama_api(endpoint, method='POST', json_data=None, stream=False):
        url = f"{OLLAMA_SERVER_URL}{endpoint}"
        headers = {'Content-Type': 'application/json'}
        try:
            if method == 'POST':
                response = requests.post(url, headers=headers, json=json_data, stream=stream)
            elif method == 'GET':
                response = requests.get(url, headers=headers, stream=stream)
            else:
                raise ValueError("Unsupported HTTP method")
            response.raise_for_status()
            if stream:
                return response.iter_content(chunk_size=8192)
            return response.json()
        except requests.exceptions.RequestException as e:
            raise HTTPException(status_code=500, detail=f"Ollama API error: {e}")

    # --- 2. Defina TODAS as tools na instância 'mcp' ---
    # [ ... todas as suas 12 funções @mcp.tool ... ]
    # [ ... (não preciso repetir todas aqui, elas estão corretas) ... ]

    @mcp.tool(
        name="generate_completion",
        description="Gera uma conclusão para um dado prompt com um modelo Ollama."
    )
    async def generate_completion(model: str, prompt: str, stream: bool = False, options: Optional[Dict[str, Any]] = None):
        payload = {"model": model, "prompt": prompt, "stream": stream}
        if options: payload["options"] = options
        return call_ollama_api("/api/generate", json_data=payload, stream=stream)

    @mcp.tool(
        name="generate_chat_completion",
        description="Gera a próxima mensagem em um chat com um modelo Ollama, mantendo o histórico de mensagens."
    )
    async def generate_chat_completion(model: str, messages: List[Message], stream: bool = False, options: Optional[Dict[str, Any]] = None):
        payload = {"model": model, "messages": [msg.dict(exclude_none=True) for msg in messages], "stream": stream}
        if options: payload["options"] = options
        return call_ollama_api("/api/chat", json_data=payload, stream=stream)

    @mcp.tool(
        name="create_model",
        description="Cria um modelo Ollama a partir de outro modelo, diretório safetensors ou arquivo GGUF."
    )
    async def create_model(model: str, from_model: Optional[str] = None, quantize: Optional[str] = None, system: Optional[str] = None):
        payload = {"model": model}
        if from_model: payload["from"] = from_model
        if quantize: payload["quantize"] = quantize
        if system: payload["system"] = system
        return call_ollama_api("/api/create", json_data=payload)

    @mcp.tool(
        name="list_local_models",
        description="Lista os modelos Ollama disponíveis localmente."
    )
    async def list_local_models():
        return call_ollama_api("/api/tags", method='GET')

    @mcp.tool(
        name="show_model_information",
        description="Mostra informações sobre um modelo Ollama, incluindo detalhes, modelfile, template, parâmetros, licença e prompt do sistema."
    )
    async def show_model_information(model: str, verbose: bool = False):
        payload = {"model": model, "verbose": verbose}
        return call_ollama_api("/api/show", json_data=payload)

    @mcp.tool(
        name="copy_model",
        description="Copia um modelo Ollama existente para um novo nome."
    )
    async def copy_model(source_model: str, destination_model: str):
        payload = {"source": source_model, "destination": destination_model}
        return call_ollama_api("/api/copy", json_data=payload)

    @mcp.tool(
        name="delete_model",
        description="Exclui um modelo Ollama."
    )
    async def delete_model(model: str):
        payload = {"name": model}
        return call_ollama_api("/api/delete", method='DELETE', json_data=payload)

    @mcp.tool(
        name="pull_model",
        description="Baixa um modelo Ollama do registro."
    )
    async def pull_model(model: str, insecure: bool = False, stream: bool = False):
        payload = {"name": model, "insecure": insecure, "stream": stream}
        return call_ollama_api("/api/pull", json_data=payload, stream=stream)

    @mcp.tool(
        name="push_model",
        description="Envia um modelo Ollama para um registro."
    )
    async def push_model(model: str, insecure: bool = False, stream: bool = False):
        payload = {"name": model, "insecure": insecure, "stream": stream}
        return call_ollama_api("/api/push", json_data=payload, stream=stream)

    @mcp.tool(
        name="generate_embeddings",
        description="Gera embeddings a partir de um modelo Ollama."
    )
    async def generate_embeddings(model: str, prompt: str):
        payload = {"model": model, "prompt": prompt}
        return call_ollama_api("/api/embeddings", json_data=payload)

    @mcp.tool(
        name="list_running_models",
        description="Lista os modelos Ollama que estão atualmente carregados na memória."
    )
    async def list_running_models():
        return call_ollama_api("/api/ps", method='GET')

    @mcp.tool(
        name="get_ollama_version",
        description="Recupera a versão do Ollama."
    )
    async def get_ollama_version():
        return call_ollama_api("/api/version", method='GET')


    # --- 3. CRIE o app MCP (mcp_app) com a rota raiz ---
    mcp_app = mcp.http_app(path="/", stateless=True)

    # --- 4. CRIE o app FastAPI principal, passando o lifespan do mcp_app ---
    app = FastAPI(lifespan=mcp_app.lifespan)

    # --- 5. MONTE o mcp_app no app principal ---
    app.mount("/mcp", mcp_app)

    # --- 6. (Opcional) Rota raiz no app principal para health check ---
    @app.get("/")
    async def read_root():
        return {"message": "FastAPI root. MCP server is at /mcp/"}


    if __name__ == "__main__":
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000)