apiVersion: v1
kind: ConfigMap
metadata:
  name: fastmcp-config
  namespace: fastmcp-server
data:
  requirements.txt: |
    fastmcp
    requests
    uvicorn
    fastapi
    pydantic
  server.py: |
    from fastapi import FastAPI, HTTPException
    from fastmcp import FastMCP
    import requests
    import os
    from typing import List, Optional, Dict, Any, Literal
    from pydantic import BaseModel, Field

    # --- 1. Crie a instância MCP ---
    mcp = FastMCP(stateless_http=True)

    OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL", "http://ollama.local")

    # --- 2. Modelos Pydantic (COM AJUSTE PARA IGNORAR EXTRAS DO N8N) ---
    
    class BaseIgnorer(BaseModel):
        # Esta config permite que o n8n envie 'sessionId', 'action', etc sem quebrar o modelo
        model_config = {"extra": "ignore"}

    class Message(BaseIgnorer):
        role: Literal["system", "user", "assistant", "tool"] = Field(..., description="O papel do autor da mensagem.")
        content: str = Field(..., description="O conteúdo de texto da mensagem.")
        images: Optional[List[str]] = Field(default=None, description="Lista opcional de imagens em base64.")

    class GenerateCompletionInput(BaseIgnorer):
        model: str = Field(..., description="Nome do modelo (ex: 'llama3').")
        prompt: str = Field(..., description="Prompt do usuário.")
        system: Optional[str] = Field(default=None, description="Prompt do sistema.")
        template: Optional[str] = Field(default=None, description="Template de prompt.")
        context: Optional[List[int]] = Field(default=None, description="Contexto anterior.")
        stream: bool = Field(default=False, description="Stream da resposta.")
        raw: bool = Field(default=False, description="Prompt cru.")
        images: Optional[List[str]] = Field(default=None, description="Imagens base64.")
        options: Optional[Dict[str, Any]] = Field(default=None, description="Opções avançadas.")

    class GenerateChatInput(BaseIgnorer):
        model: str = Field(..., description="Nome do modelo de chat.")
        messages: List[Message] = Field(..., description="Histórico de mensagens.")
        stream: bool = Field(default=False, description="Stream da resposta.")
        options: Optional[Dict[str, Any]] = Field(default=None, description="Opções avançadas.")

    class CreateModelInput(BaseIgnorer):
        model: str = Field(..., description="Nome do novo modelo.")
        from_model: str = Field(..., description="Modelo base.")
        quantize: Optional[str] = Field(default=None, description="Nível de quantização.")
        system: Optional[str] = Field(default=None, description="Prompt de sistema padrão.")

    class ShowModelInput(BaseIgnorer):
        model: str = Field(..., description="Nome do modelo.")
        verbose: bool = Field(default=False, description="Detalhes verbose.")

    class CopyModelInput(BaseIgnorer):
        source_model: str = Field(..., description="Modelo origem.")
        destination_model: str = Field(..., description="Modelo destino.")

    class DeleteModelInput(BaseIgnorer):
        model: str = Field(..., description="Modelo a excluir.")

    class PullModelInput(BaseIgnorer):
        model: str = Field(..., description="Modelo a baixar.")
        insecure: bool = Field(default=False, description="Conexão insegura.")
        stream: bool = Field(default=False, description="Stream do download.")

    class PushModelInput(BaseIgnorer):
        model: str = Field(..., description="Modelo a enviar.")
        insecure: bool = Field(default=False, description="Conexão insegura.")
        stream: bool = Field(default=False, description="Stream do upload.")

    class GenerateEmbeddingsInput(BaseIgnorer):
        model: str = Field(..., description="Modelo de embeddings.")
        prompt: str = Field(..., description="Texto para vetor.")
        options: Optional[Dict[str, Any]] = Field(default=None, description="Opções avançadas.")


    # --- 3. Função Helper ---
    def call_ollama_api(endpoint, method='POST', json_data=None, stream=False):
        url = f"{OLLAMA_SERVER_URL}{endpoint}"
        headers = {'Content-Type': 'application/json'}
        try:
            if method == 'POST':
                response = requests.post(url, headers=headers, json=json_data, stream=stream)
            elif method == 'GET':
                response = requests.get(url, headers=headers, stream=stream)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, json=json_data)
            else:
                raise ValueError("Unsupported HTTP method")
            
            response.raise_for_status()
            
            if stream:
                return response.iter_content(chunk_size=8192)
            
            if response.status_code == 204 or not response.content:
                return {"status": "success", "statusCode": response.status_code}
                
            return response.json()
        except requests.exceptions.RequestException as e:
             # Tratamento básico de erro para não quebrar o server
            detail = str(e)
            if hasattr(e, 'response') and e.response is not None:
                try:
                    detail = e.response.json().get('error', detail)
                except:
                    pass
            raise HTTPException(status_code=500, detail=f"Ollama API error: {detail}")


    # --- 4. Definição das Ferramentas (Tools) MCP ---

    @mcp.tool(name="generate_completion")
    async def generate_completion(input_data: GenerateCompletionInput):
        payload = input_data.model_dump(exclude_none=True) # model_dump é o padrão novo do Pydantic v2
        return call_ollama_api("/api/generate", json_data=payload, stream=input_data.stream)

    @mcp.tool(name="generate_chat_completion")
    async def generate_chat_completion(input_data: GenerateChatInput):
        messages_dict = [msg.model_dump(exclude_none=True) for msg in input_data.messages]
        payload = input_data.model_dump(exclude_none=True)
        payload["messages"] = messages_dict
        return call_ollama_api("/api/chat", json_data=payload, stream=input_data.stream)

    @mcp.tool(name="create_model")
    async def create_model(input_data: CreateModelInput):
        modelfile_content = f"FROM {input_data.from_model}\n"
        if input_data.system:
            modelfile_content += f"SYSTEM \"\"\"{input_data.system}\"\"\"\n"
        if input_data.quantize:
            modelfile_content += f"PARAMETER quantize {input_data.quantize}\n"
        payload = {
            "name": input_data.model,
            "modelfile": modelfile_content,
            "stream": False
        }
        return call_ollama_api("/api/create", json_data=payload)

    # --- ATENÇÃO: Adicionado **kwargs para ignorar argumentos extras do n8n ---
    @mcp.tool(name="list_local_models")
    async def list_local_models(**kwargs: Any):
        return call_ollama_api("/api/tags", method='GET')

    @mcp.tool(name="show_model_information")
    async def show_model_information(input_data: ShowModelInput):
        payload = {"name": input_data.model, "verbose": input_data.verbose}
        return call_ollama_api("/api/show", json_data=payload)

    @mcp.tool(name="copy_model")
    async def copy_model(input_data: CopyModelInput):
        payload = {"source": input_data.source_model, "destination": input_data.destination_model}
        return call_ollama_api("/api/copy", json_data=payload)

    @mcp.tool(name="delete_model")
    async def delete_model(input_data: DeleteModelInput):
        payload = {"name": input_data.model}
        return call_ollama_api("/api/delete", method='DELETE', json_data=payload)

    @mcp.tool(name="pull_model")
    async def pull_model(input_data: PullModelInput):
        payload = {
            "name": input_data.model, 
            "insecure": input_data.insecure, 
            "stream": input_data.stream
        }
        return call_ollama_api("/api/pull", json_data=payload, stream=input_data.stream)

    @mcp.tool(name="push_model")
    async def push_model(input_data: PushModelInput):
        payload = {
            "name": input_data.model, 
            "insecure": input_data.insecure, 
            "stream": input_data.stream
        }
        return call_ollama_api("/api/push", json_data=payload, stream=input_data.stream)

    @mcp.tool(name="generate_embeddings")
    async def generate_embeddings(input_data: GenerateEmbeddingsInput):
        payload = input_data.model_dump(exclude_none=True)
        return call_ollama_api("/api/embeddings", json_data=payload)

    # --- ATENÇÃO: Adicionado **kwargs ---
    @mcp.tool(name="list_running_models")
    async def list_running_models(**kwargs: Any):
        return call_ollama_api("/api/ps", method='GET')

    # --- ATENÇÃO: Adicionado **kwargs ---
    @mcp.tool(name="get_ollama_version")
    async def get_ollama_version(**kwargs: Any):
        return call_ollama_api("/api/version", method='GET')


    # --- 5. Apps e Rotas ---
    mcp_app = mcp.http_app(path="/") 

    app = FastAPI(
        title="Servidor FastMCP para Ollama",
        lifespan=mcp_app.lifespan
    )
    app.mount("/mcp", mcp_app)

    @app.get("/")
    async def read_root():
        return {"message": "Servidor FastMCP operacional."}

    if __name__ == "__main__":
        import uvicorn
        print(f"Iniciando servidor, conectando ao Ollama em: {OLLAMA_SERVER_URL}")
        uvicorn.run(app, host="0.0.0.0", port=8000)