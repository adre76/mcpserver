apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  requirements.txt: |
    fastmcp
    httpx
    pydantic
  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional
    from fastmcp import FastMCP
    from pydantic import BaseModel, Field

    mcp = FastMCP(name="Ollama Tools Server")
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")

    # Schema Pydantic para cada endpoint
    class OllamaGenerateInput(BaseModel):
        model: str = Field(description="Nome do modelo (ex: 'llama3:latest')")
        prompt: str = Field(description="Prompt para gerar texto")
        suffix: Optional[str] = None
        images: Optional[List[str]] = None  # base64 encoded images
        format: Optional[str] = None
        options: Optional[Dict[str, Any]] = None
        system: Optional[str] = None
        template: Optional[str] = None
        stream: Optional[bool] = False
        raw: Optional[bool] = False
        keep_alive: Optional[str] = None
        context: Optional[Any] = None

    class OllamaChatInput(BaseModel):
        model: str = Field(description="Nome do modelo (ex: 'llama3:latest')")
        messages: List[Dict[str, str]] = Field(description="Lista de mensagens com papéis")
        tools: Optional[List[Dict[str, Any]]] = None
        format: Optional[Any] = None
        options: Optional[Dict[str, Any]] = None
        stream: Optional[bool] = False
        keep_alive: Optional[str] = None

    class OllamaCreateInput(BaseModel):
        name: str = Field(description="Nome para novo modelo")
        modelfile: str = Field(description="Conteúdo do Modelfile")

    class OllamaModelInfoInput(BaseModel):
        name: str = Field(description="Nome do modelo")

    class OllamaCopyInput(BaseModel):
        source: str = Field(description="Nome do modelo origem")
        destination: str = Field(description="Nome para a cópia")

    class OllamaDeleteInput(BaseModel):
        name: str = Field(description="Nome do modelo a deletar")

    class OllamaPullInput(BaseModel):
        name: str = Field(description="Nome do modelo para puxar")
        insecure: Optional[bool] = False

    class OllamaPushInput(BaseModel):
        name: str = Field(description="Nome do modelo para enviar")
        insecure: Optional[bool] = False

    class OllamaEmbeddingsInput(BaseModel):
        model: str = Field(description="Nome do modelo de embedding")
        prompt: str = Field(description="Texto para embedding")
        options: Optional[Dict[str, Any]] = None

    # Classe que implementa as ferramentas MCP ligadas aos endpoints Ollama
    class OllamaTools:
        async def _handle_ollama_request(self, method: str, path: str, payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
            if method == "GET":
                response = await ollama_client.get(path)
            elif method == "POST":
                response = await ollama_client.post(path, json=payload)
            elif method == "DELETE":
                response = await ollama_client.request("DELETE", path, json=payload)
            else:
                raise ValueError(f"Método HTTP não suportado: {method}")
            response.raise_for_status()
            return response.json()

        async def ping(self) -> Dict[str, Any]:
            print("Ping tool executado")
            return {"pong": True}

        async def generate(self, data: OllamaGenerateInput) -> Dict[str, Any]:
            payload = data.model_dump(exclude_none=True)
            return await self._handle_ollama_request("POST", "/api/generate", payload)

        async def chat(self, data: OllamaChatInput) -> Dict[str, Any]:
            payload = data.model_dump(exclude_none=True)
            return await self._handle_ollama_request("POST", "/api/chat", payload)

        async def create_model(self, data: OllamaCreateInput) -> Dict[str, Any]:
            payload = data.model_dump()
            return await self._handle_ollama_request("POST", "/api/create", payload)

        async def list_models(self) -> Dict[str, Any]:
            return await self._handle_ollama_request("GET", "/api/tags")

        async def show_model_info(self, data: OllamaModelInfoInput) -> Dict[str, Any]:
            return await self._handle_ollama_request("POST", "/api/show", data.model_dump())

        async def copy_model(self, data: OllamaCopyInput) -> Dict[str, Any]:
            return await self._handle_ollama_request("POST", "/api/copy", data.model_dump())

        async def delete_model(self, data: OllamaDeleteInput) -> Dict[str, Any]:
            return await self._handle_ollama_request("DELETE", "/api/delete", data.model_dump())

        async def pull_model(self, data: OllamaPullInput) -> Dict[str, Any]:
            payload = data.model_dump(exclude_none=True)
            return await self._handle_ollama_request("POST", "/api/pull", payload)

        async def push_model(self, data: OllamaPushInput) -> Dict[str, Any]:
            payload = data.model_dump(exclude_none=True)
            return await self._handle_ollama_request("POST", "/api/push", payload)

        async def generate_embeddings(self, data: OllamaEmbeddingsInput) -> Dict[str, Any]:
            payload = data.model_dump(exclude_none=True)
            return await self._handle_ollama_request("POST", "/api/embeddings", payload)

        async def list_running_models(self) -> Dict[str, Any]:
            return await self._handle_ollama_request("GET", "/api/ps")

        async def version(self) -> Dict[str, Any]:
            return await self._handle_ollama_request("GET", "/api/version")

    tools = OllamaTools()

    print("Iniciando registro dos tools MCP...")
    mcp.tool(tools.ping)
    mcp.tool(tools.generate)
    mcp.tool(tools.chat)
    mcp.tool(tools.create_model)
    mcp.tool(tools.list_models)
    mcp.tool(tools.show_model_info)
    mcp.tool(tools.copy_model)
    mcp.tool(tools.delete_model)
    mcp.tool(tools.pull_model)
    mcp.tool(tools.push_model)
    mcp.tool(tools.generate_embeddings)
    mcp.tool(tools.list_running_models)
    mcp.tool(tools.version)
    print("Registro dos tools MCP finalizado")

    if __name__ == "__main__":
        print("Iniciando servidor FastMCP HTTP via mcp.run()...")
        mcp.run(transport="http", host="0.0.0.0", port=8000, path="/mcp")
