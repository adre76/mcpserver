apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  requirements.txt: |
    fastmcp # Puxa fastapi, uvicorn, etc. como dependências
    httpx
    pydantic

  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional
    # Importação correta conforme a documentação
    from fastmcp import FastMCP
    from pydantic import BaseModel, Field
    import sys

    # Cria a instância principal do FastMCP
    mcp = FastMCP(name="Ollama Tools Server")

    # Configuração do cliente Ollama
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")
    try:
        ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)
        print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")
    except Exception as e:
        print(f"ERRO ao criar cliente HTTP para Ollama: {e}")
        sys.exit("Falha ao inicializar cliente Ollama")


    # --- Schemas Pydantic para inputs das tools ---
    # (Mantidos iguais à sua versão)
    class OllamaGenerateInput(BaseModel):
        model: str = Field(description="Nome do modelo (ex:'llama3:latest')")
        prompt: str = Field(description="Prompt para a geração")
        # Removido 'suffix' que não existe mais na API Ollama recente
        images: Optional[List[str]] = None
        format: Optional[str] = None
        options: Optional[Dict[str, Any]] = None
        system: Optional[str] = None
        template: Optional[str] = None
        stream: Optional[bool] = False # Forçaremos False na função
        raw: Optional[bool] = False
        keep_alive: Optional[str] = None
        # Context foi depreciado, mas mantido por compatibilidade se necessário
        context: Optional[Any] = None

    class OllamaChatInput(BaseModel):
        model: str = Field(description="Nome do modelo")
        messages: List[Dict[str, str]] = Field(description="Lista de mensagens com roles como 'user', 'assistant'")
        tools: Optional[List[Dict[str, Any]]] = None # Para Ollama function calling
        format: Optional[str] = None # 'json' por exemplo
        options: Optional[Dict[str, Any]] = None
        stream: Optional[bool] = False # Forçaremos False na função
        keep_alive: Optional[str] = None

    class OllamaCreateInput(BaseModel): name: str = Field(description="Nome do novo modelo"); modelfile: str = Field(description="Conteúdo do Modelfile")
    class OllamaModelInfoInput(BaseModel): name: str = Field(description="Nome do modelo")
    class OllamaCopyInput(BaseModel): source: str = Field(description="Nome do modelo fonte"); destination: str = Field(description="Nome para a cópia")
    class OllamaDeleteInput(BaseModel): name: str = Field(description="Nome do modelo a ser deletado")
    class OllamaPullInput(BaseModel): name: str = Field(description="Nome do modelo para puxar"); insecure: Optional[bool] = False; stream: Optional[bool] = False # Adicionado stream
    class OllamaPushInput(BaseModel): name: str = Field(description="Nome do modelo para enviar"); insecure: Optional[bool] = False; stream: Optional[bool] = False # Adicionado stream
    class OllamaEmbeddingsInput(BaseModel): model: str = Field(description="Nome do modelo embedding"); prompt: str = Field(description="Texto para vetorização"); options: Optional[Dict[str, Any]] = None


    # --- Função Auxiliar para Chamadas Ollama ---
    # (Não precisa ser método de classe)
    async def _handle_ollama_request(method: str, path: str, payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Função interna para fazer requisições à API do Ollama e tratar erros."""
        try:
            print(f"-> Chamando Ollama: {method} {path} Payload: {payload is not None}")
            if method == "GET": response = await ollama_client.get(path)
            elif method == "POST": response = await ollama_client.post(path, json=payload)
            elif method == "DELETE": response = await ollama_client.request("DELETE", path, json=payload) # Usar request para DELETE com body
            else: return {"error": f"MCP Tool: Método HTTP não suportado: {method}"}

            response.raise_for_status() # Lança exceção para erros 4xx/5xx

            # Tratamento para respostas sem conteúdo (ex: DELETE bem-sucedido)
            if response.status_code == 200 and not response.content and method == "DELETE":
                print(f"<- Resposta Ollama: Sucesso (sem corpo)")
                return {"status": "success"}
            
            # Tratamento para API /api/version que retorna texto puro
            if path == "/api/version":
                 version_text = response.text.strip().replace('"', '') # Limpa aspas se houver
                 print(f"<- Resposta Ollama: Versão '{version_text}'")
                 return {"version": version_text}

            # Para Pull/Push/Create, Ollama pode retornar stream mesmo com stream=false.
            # Vamos ler a última linha do stream se for o caso.
            if path in ["/api/pull", "/api/push", "/api/create"] and response.headers.get("content-type") == "application/x-ndjson":
                 print("<- Resposta Ollama: Stream NDJSON detectado, lendo última linha...")
                 lines = await response.aread()
                 last_line = lines.strip().split(b'\n')[-1]
                 import json
                 try:
                     result = json.loads(last_line)
                     print(f"<- Resposta Ollama (última linha do stream): {result}")
                     return result
                 except json.JSONDecodeError:
                      print(f"<- ERRO: Falha ao decodificar última linha do stream: {last_line}")
                      return {"error": "Falha ao processar resposta stream do Ollama", "last_line": last_line.decode('utf-8', errors='ignore')}


            result = response.json()
            print(f"<- Resposta Ollama: {str(result)[:200]}...") # Log truncado
            return result

        except httpx.ConnectError as e:
            print(f"<- ERRO: Conexão com Ollama falhou: {e}")
            return {"error": f"MCP Tool: Não foi possível conectar ao Ollama em {OLLAMA_BASE_URL}", "details": str(e)}
        except httpx.HTTPStatusError as e:
            error_details = e.response.text or "Sem detalhes"
            print(f"<- ERRO: Ollama retornou erro {e.response.status_code}: {error_details}")
            return {"error": f"MCP Tool: Ollama retornou um erro {e.response.status_code}", "details": error_details}
        except Exception as e:
            print(f"<- ERRO: Erro inesperado ao processar {path}: {e}")
            return {"error": f"MCP Tool: Erro inesperado ao processar {path}", "details": str(e)}


    # --- Definição das Tools com Decorator ---

    print("-> Registrando tool: ping")
    @mcp.tool()
    async def ping() -> Dict[str, Any]:
        """Verifica se o servidor MCP está online."""
        print("Executando tool: ping")
        return {"pong": True}

    print("-> Registrando tool: ollama_generate")
    @mcp.tool()
    async def ollama_generate(data: OllamaGenerateInput) -> Dict[str, Any]:
        """Gera texto a partir de um prompt usando a API /api/generate do Ollama (sem stream)."""
        print(f"Executando tool: ollama_generate (Modelo: {data.model})")
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False # Garantir que não use stream
        return await _handle_ollama_request("POST", "/api/generate", payload)

    print("-> Registrando tool: ollama_chat")
    @mcp.tool()
    async def ollama_chat(data: OllamaChatInput) -> Dict[str, Any]:
        """Continua uma conversa usando a API /api/chat do Ollama (sem stream)."""
        print(f"Executando tool: ollama_chat (Modelo: {data.model})")
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False # Garantir que não use stream
        return await _handle_ollama_request("POST", "/api/chat", payload)

    print("-> Registrando tool: ollama_create_model")
    @mcp.tool()
    async def ollama_create_model(data: OllamaCreateInput) -> Dict[str, Any]:
        """Cria um modelo Ollama a partir de um Modelfile usando a API /api/create."""
        print(f"Executando tool: ollama_create_model (Nome: {data.name})")
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False # A API pode retornar stream, mas queremos o status final
        return await _handle_ollama_request("POST", "/api/create", payload)

    print("-> Registrando tool: ollama_list_models")
    @mcp.tool()
    async def ollama_list_models() -> Dict[str, Any]:
        """Lista os modelos Ollama disponíveis localmente (API /api/tags)."""
        print("Executando tool: ollama_list_models")
        return await _handle_ollama_request("GET", "/api/tags")

    print("-> Registrando tool: ollama_show_model_info")
    @mcp.tool()
    async def ollama_show_model_info(data: OllamaModelInfoInput) -> Dict[str, Any]:
        """Mostra informações detalhadas de um modelo Ollama (API /api/show)."""
        print(f"Executando tool: ollama_show_model_info (Modelo: {data.name})")
        return await _handle_ollama_request("POST", "/api/show", data.model_dump())

    print("-> Registrando tool: ollama_copy_model")
    @mcp.tool()
    async def ollama_copy_model(data: OllamaCopyInput) -> Dict[str, Any]:
        """Copia um modelo Ollama local (API /api/copy)."""
        print(f"Executando tool: ollama_copy_model (Origem: {data.source}, Destino: {data.destination})")
        return await _handle_ollama_request("POST", "/api/copy", data.model_dump())

    print("-> Registrando tool: ollama_delete_model")
    @mcp.tool()
    async def ollama_delete_model(data: OllamaDeleteInput) -> Dict[str, Any]:
        """Deleta um modelo Ollama local (API /api/delete)."""
        print(f"Executando tool: ollama_delete_model (Modelo: {data.name})")
        # Ollama API expects model name in JSON body for DELETE
        return await _handle_ollama_request("DELETE", "/api/delete", data.model_dump())

    print("-> Registrando tool: ollama_pull_model")
    @mcp.tool()
    async def ollama_pull_model(data: OllamaPullInput) -> Dict[str, Any]:
        """Baixa (pull) um modelo Ollama do registro (API /api/pull)."""
        print(f"Executando tool: ollama_pull_model (Modelo: {data.name})")
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False # A API pode retornar stream, mas queremos o status final
        return await _handle_ollama_request("POST", "/api/pull", payload)

    print("-> Registrando tool: ollama_push_model")
    @mcp.tool()
    async def ollama_push_model(data: OllamaPushInput) -> Dict[str, Any]:
        """Envia (push) um modelo Ollama local para o registro (API /api/push)."""
        print(f"Executando tool: ollama_push_model (Modelo: {data.name})")
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False # A API pode retornar stream, mas queremos o status final
        return await _handle_ollama_request("POST", "/api/push", payload)

    print("-> Registrando tool: ollama_generate_embeddings")
    @mcp.tool()
    async def ollama_generate_embeddings(data: OllamaEmbeddingsInput) -> Dict[str, Any]:
        """Gera embeddings para um texto usando um modelo Ollama (API /api/embeddings)."""
        print(f"Executando tool: ollama_generate_embeddings (Modelo: {data.model})")
        payload = data.model_dump(exclude_none=True)
        return await _handle_ollama_request("POST", "/api/embeddings", payload)

    print("-> Registrando tool: ollama_list_running_models")
    @mcp.tool()
    async def ollama_list_running_models() -> Dict[str, Any]:
        """Lista os modelos Ollama atualmente carregados na memória (API /api/ps)."""
        print("Executando tool: ollama_list_running_models")
        return await _handle_ollama_request("GET", "/api/ps")

    print("-> Registrando tool: ollama_version")
    @mcp.tool()
    async def ollama_version() -> Dict[str, Any]:
        """Obtém a versão do servidor Ollama (API /api/version)."""
        print("Executando tool: ollama_version")
        return await _handle_ollama_request("GET", "/api/version")

    print("-> Registro de todas as tools concluído.")

    # --- Bloco para iniciar o servidor ---
    if __name__ == "__main__":
        try:
            print("-> Iniciando servidor FastMCP HTTP via mcp.run() na porta 8000...")
            # Removendo path="/mcp" para seguir exemplos da documentação
            mcp.run(transport="http", host="0.0.0.0", port=8000)
        except Exception as e:
            print(f"ERRO FATAL ao iniciar servidor mcp.run(): {e}")
            sys.exit(f"Falha ao iniciar mcp.run(): {e}")