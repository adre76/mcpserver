apiVersion: v1
kind: ConfigMap
metadata:
  name: mcp-app-code
  namespace: mcp-server
data:
  # Arquivo de dependências do Python
  requirements.txt: |
    # --- CORREÇÃO ---
    # Instala tanto os extras do 'server' quanto do 'cli'
    mcp[server,cli]
    httpx

  # Servidor MCP com o conjunto completo de tools do Ollama
  main.py: |
    import os
    import httpx
    from typing import List, Dict, Any, Optional

    from mcp.server.fastmcp import FastMCP
    from pydantic import BaseModel, Field
    
    mcp = FastMCP("Ollama Tools Server")
    
    # Pega a URL do Ollama da variável de ambiente
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama.local")

    # Cria um cliente HTTP assíncrono para se comunicar com o Ollama
    ollama_client = httpx.AsyncClient(base_url=OLLAMA_BASE_URL, timeout=300.0)

    print(f"MCP Server (Ollama Full API) iniciado. Apontando para: {OLLAMA_BASE_URL}")

    # --- Schemas de Input (Pydantic) para as Tools ---
    
    class OllamaGenerateInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        prompt: str = Field(description="O prompt para gerar uma resposta.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")
        system: Optional[str] = Field(None, description="A instrução de sistema (system prompt).")
        template: Optional[str] = Field(None, description="O template de prompt a ser usado (sobrescreve o do modelo).")
        context: Optional[str] = Field(None, description="O contexto de uma conversa anterior (para manter estado).")

    class OllamaChatInput(BaseModel):
        model: str = Field(description="O nome do modelo a ser usado (ex: 'llama3:latest').")
        messages: List[Dict[str, str]] = Field(description="Lista de mensagens no formato {'role': 'user', 'content': '...'}.")
        format: Optional[str] = Field(None, description="O formato da resposta (ex: 'json').")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo (ex: 'temperature').")

    class OllamaCreateInput(BaseModel):
        name: str = Field(description="O nome do novo modelo a ser criado (ex: 'meu-modelo:latest').")
        modelfile: str = Field(description="O conteúdo completo do 'Modelfile' como uma string.")

    class OllamaModelInfoInput(BaseModel):
        name: str = Field(description="O nome do modelo para inspecionar.")

    class OllamaCopyInput(BaseModel):
        source: str = Field(description="O nome do modelo de origem (ex: 'llama3').")
        destination: str = Field(description="O novo nome para a cópia (ex: 'llama3-backup').")

    class OllamaDeleteInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser excluído permanentemente.")

    class OllamaPullInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser baixado do registro (ex: 'llama3:latest').")
        insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")

    class OllamaPushInput(BaseModel):
        name: str = Field(description="O nome do modelo a ser enviado para o registro (ex: 'meu-namespace/meu-modelo').")
        insecure: Optional[bool] = Field(None, description="Permitir conexões 'inseguras' (http) com o registro.")

    class OllamaEmbeddingsInput(BaseModel):
        model: str = Field(description="O nome do modelo de embedding a ser usado.")
        prompt: str = Field(description="O texto a ser convertido em um vetor (embedding).")
        options: Optional[Dict[str, Any]] = Field(None, description="Parâmetros avançados do modelo.")

    # --- Bloco Genérico de Tratamento de Erro ---
    async def _handle_ollama_request(
        method: str, 
        path: str, 
        payload: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        try:
            if method == "GET":
                response = await ollama_client.get(path)
            elif method == "POST":
                response = await ollama_client.post(path, json=payload)
            elif method == "DELETE":
                response = await ollama_client.request("DELETE", path, json=payload)
            else:
                return {"error": f"MCP Tool: Método HTTP não suportado: {method}"}

            response.raise_for_status() 

            if response.status_code == 200 and not response.content:
                return {"status": "success"}
            
            if path == "/api/version":
                return {"version": response.text.strip()}

            return response.json()
            
        except httpx.ConnectError as e:
            return {"error": f"MCP Tool: Não foi possível conectar ao Ollama em {OLLAMA_BASE_URL}", "details": str(e)}
        except httpx.HTTPStatusError as e:
            return {"error": f"MCP Tool: Ollama retornou um erro {e.response.status_code}", "details": e.response.text or "Sem detalhes"}
        except Exception as e:
            return {"error": f"MCP Tool: Erro inesperado ao processar {path}", "details": str(e)}

    # --- Definição das Tools ---
    # (Usando @mcp.tool)

    @mcp.tool(
        name="ollama_generate",
        description="Gera uma conclusão de texto (completion) a partir de um único prompt. Ideal para tarefas de geração de texto simples, sem histórico de conversa. Sempre retorna uma resposta JSON única (sem stream)."
    )
    async def ollama_generate(data: OllamaGenerateInput) -> Dict[str, Any]:
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/generate", payload)

    @mcp.tool(
        name="ollama_chat",
        description="Gera uma resposta de chat baseada em um histórico de mensagens. Use para continuar conversas. Sempre retorna uma resposta JSON única (sem stream)."
    )
    async def ollama_chat(data: OllamaChatInput) -> Dict[str, Any]:
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/chat", payload)

    @mcp.tool(
        name="ollama_create_model",
        description="Cria um novo modelo Ollama customizado a partir do conteúdo de um 'Modelfile'. O 'Modelfile' deve ser passado como uma string completa."
    )
    async def ollama_create_model(data: OllamaCreateInput) -> Dict[str, Any]:
        payload = data.model_dump()
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/create", payload)

    @mcp.tool(
        name="ollama_list_models",
        description="Lista todos os modelos de linguagem que foram baixados e estão disponíveis localmente no servidor Ollama (equivalente a /api/tags)."
    )
    async def ollama_list_models() -> Dict[str, Any]:
        return await _handle_ollama_request("GET", "/api/tags")

    @mcp.tool(
        name="ollama_show_model_info",
        description="Exibe informações detalhadas sobre um modelo específico, como seu 'Modelfile' original, parâmetros, e template de sistema."
    )
    async def ollama_show_model_info(data: OllamaModelInfoInput) -> Dict[str, Any]:
        return await _handle_ollama_request("POST", "/api/show", data.model_dump())

    @mcp.tool(
        name="ollama_copy_model",
        description="Cria uma cópia de um modelo local existente, dando-lhe um novo nome. Útil para criar 'snapshots' ou testar variações de um modelo."
    )
    async def ollama_copy_model(data: OllamaCopyInput) -> Dict[str, Any]:
        return await _handle_ollama_request("POST", "/api/copy", data.model_dump())

    @mcp.tool(
        name="ollama_delete_model",
        description="Exclui permanentemente um modelo do servidor Ollama local. Use com cuidado, pois isso libera espaço em disco."
    )
    async def ollama_delete_model(data: OllamaDeleteInput) -> Dict[str, Any]:
        return await _handle_ollama_request("DELETE", "/api/delete", data.model_dump())

    @mcp.tool(
        name="ollama_pull_model",
        description="Baixa um modelo de um registro (como o Ollama Hub) para o servidor local. Retorna o status JSON quando concluído (sem stream)."
    )
    async def ollama_pull_model(data: OllamaPullInput) -> Dict[str, Any]:
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/pull", payload)

    @mcp.tool(
        name="ollama_push_model",
        description="Envia um modelo local para um registro remoto. O modelo deve ter um nome de namespace (ex: 'meu_usuario/meu_modelo'). Retorna o status JSON quando concluído."
    )
    async def ollama_push_model(data: OllamaPushInput) -> Dict[str, Any]:
        payload = data.model_dump(exclude_none=True)
        payload["stream"] = False
        return await _handle_ollama_request("POST", "/api/push", payload)

    @mcp.tool(
        name="ollama_generate_embeddings",
        description="Gera representações vetoriais (embeddings) para um determinado texto (prompt) usando um modelo específico. Essencial para tarefas de RAG e busca semântica."
    )
    async def ollama_generate_embeddings(data: OllamaEmbeddingsInput) -> Dict[str, Any]:
        payload = data.model_dump(exclude_none=True)
        return await _handle_ollama_request("POST", "/api/embeddings", payload)

    @mcp.tool(
        name="ollama_list_running_models",
        description="Verifica e lista quais modelos estão atualmente carregados na memória (VRAM/RAM) do servidor Ollama e há quanto tempo estão ativos (equivalente a /api/ps)."
    )
    async def ollama_list_running_models() -> Dict[str, Any]:
        return await _handle_ollama_request("GET", "/api/ps")

    @mcp.tool(
        name="ollama_get_version",
        description="Retorna a string de versão exata do servidor Ollama em execução."
    )
    async def ollama_get_version() -> Dict[str, Any]:
        return await _handle_ollama_request("GET", "/api/version")